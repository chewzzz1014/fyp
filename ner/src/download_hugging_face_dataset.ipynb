{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chewzzz1014/fyp/blob/master/ner/src/download_hugging_face_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c8ce31",
      "metadata": {
        "id": "a1c8ce31"
      },
      "source": [
        "# Dataset Preparation\n",
        "Steps:\n",
        "1. Download multiple resume dataset from hugging face\n",
        "2. Remove unwanted columns and limit number of rows (for dataset with many rows, only focus on the IT-related resumes)\n",
        "3. Merge all datasets into one csv and download.\n",
        "\n",
        "Dataset Links\n",
        "- https://huggingface.co/datasets/InferencePrince555/Resume-Dataset\n",
        "- https://huggingface.co/datasets/kvsrkc/Resume\n",
        "- https://huggingface.co/datasets/opensporks/resumes\n",
        "- https://huggingface.co/datasets/Dh1raj/resume-dataset\n",
        "- https://huggingface.co/datasets/cnamuangtoun/resume-job-description-fit?row=19\n",
        "- https://huggingface.co/datasets/ahmedheakl/resume-atlas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "896a477a",
      "metadata": {
        "id": "896a477a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9NZYbBHxdGu8",
        "outputId": "fe69f471-0ad2-4b80-f73f-a4215195758c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9NZYbBHxdGu8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc52b6e2",
      "metadata": {
        "id": "fc52b6e2",
        "outputId": "650a5556-4b59-4a03-dc39-07c910add0d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# import dataset from hugging face\n",
        "df1 = pd.read_csv(\"hf://datasets/InferencePrince555/Resume-Dataset/updated_data_final_cleaned.csv\")\n",
        "# df2 = pd.read_csv(\"hf://datasets/kvsrkc/Resume/Resume.csv\")\n",
        "# df3 = pd.read_csv(\"hf://datasets/opensporks/resumes/Resume/Resume.csv\")\n",
        "\n",
        "# df4_splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet', 'validation': 'data/validation-00000-of-00001.parquet'}\n",
        "# df4 = pd.read_parquet(\"hf://datasets/Dh1raj/resume-dataset/\" + df4_splits[\"train\"])\n",
        "\n",
        "# df5_splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
        "# df5 = pd.read_csv(\"hf://datasets/cnamuangtoun/resume-job-description-fit/\" + df5_splits[\"train\"])\n",
        "\n",
        "df6 = pd.read_csv(\"hf://datasets/ahmedheakl/resume-atlas/train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7720f610",
      "metadata": {
        "id": "7720f610"
      },
      "source": [
        "## Preprocess df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2218e22",
      "metadata": {
        "id": "c2218e22",
        "outputId": "8c8785b3-22d1-40df-c0a8-8f16be8e8dc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct values in 'instruction' column in df1: ['Generate a Resume for a Accountant Job'\n",
            " 'Generate a Resume for a Advocate Job'\n",
            " 'Generate a Resume for a Agriculture Job'\n",
            " 'Generate a Resume for a Apparel Job' 'Generate a Resume for a Arts Job'\n",
            " 'Generate a Resume for a Automation Testing Job'\n",
            " 'Generate a Resume for a Automobile Job'\n",
            " 'Generate a Resume for a Aviation Job' 'Generate a Resume for a BPO Job'\n",
            " 'Generate a Resume for a Banking Job'\n",
            " 'Generate a Resume for a Blockchain Job'\n",
            " 'Generate a Resume for a Business Analyst Job'\n",
            " 'Generate a Resume for a Business Development Job'\n",
            " 'Generate a Resume for a Chef Job'\n",
            " 'Generate a Resume for a Civil Engineer Job'\n",
            " 'Generate a Resume for a Construction Job'\n",
            " 'Generate a Resume for a Consultant Job'\n",
            " 'Generate a Resume for a Data Science Job'\n",
            " 'Generate a Resume for a Database Job'\n",
            " 'Generate a Resume for a Database Administrator Job'\n",
            " 'Generate a Resume for a Designer Job'\n",
            " 'Generate a Resume for a DevOps Engineer Job'\n",
            " 'Generate a Resume for a Digital Media Job'\n",
            " 'Generate a Resume for a DotNet Developer Job'\n",
            " 'Generate a Resume for a ETL Developer Job'\n",
            " 'Generate a Resume for a Electrical Engineering Job'\n",
            " 'Generate a Resume for a Engineering Job'\n",
            " 'Generate a Resume for a Finance Job'\n",
            " 'Generate a Resume for a Fitness Job' 'Generate a Resume for a HR Job'\n",
            " 'Generate a Resume for a Hadoop Job'\n",
            " 'Generate a Resume for a Health and fitness Job'\n",
            " 'Generate a Resume for a Healthcare Job'\n",
            " 'Generate a Resume for a Information Technology Job'\n",
            " 'Generate a Resume for a Java Developer Job'\n",
            " 'Generate a Resume for a Mechanical Engineer Job'\n",
            " 'Generate a Resume for a Network Administrator Job'\n",
            " 'Generate a Resume for a Network Security Engineer Job'\n",
            " 'Generate a Resume for a Operations Manager Job'\n",
            " 'Generate a Resume for a PMO Job'\n",
            " 'Generate a Resume for a Project manager Job'\n",
            " 'Generate a Resume for a Public Relations Job'\n",
            " 'Generate a Resume for a Python Developer Job'\n",
            " 'Generate a Resume for a SAP Developer Job'\n",
            " 'Generate a Resume for a Sales Job'\n",
            " 'Generate a Resume for a Security Analyst Job'\n",
            " 'Generate a Resume for a Software Developer Job'\n",
            " 'Generate a Resume for a Systems Administrator Job'\n",
            " 'Generate a Resume for a Teacher Job'\n",
            " 'Generate a Resume for a Testing Job'\n",
            " 'Generate a Resume for a Web Designing Job'\n",
            " 'Generate a Resume for a Web Developer Job']\n"
          ]
        }
      ],
      "source": [
        "# print distinct instruction\n",
        "distinct_instructions_df1 = df1['instruction'].unique()\n",
        "print(\"Distinct values in 'instruction' column in df1:\", distinct_instructions_df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f534f2",
      "metadata": {
        "id": "17f534f2",
        "outputId": "a6d687fd-6a0e-4dba-a362-4c0b35a9ed9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct values in 'instruction' column of filtered_df: ['Generate a Resume for a Data Science Job'\n",
            " 'Generate a Resume for a Database Job'\n",
            " 'Generate a Resume for a Database Administrator Job'\n",
            " 'Generate a Resume for a DevOps Engineer Job'\n",
            " 'Generate a Resume for a DotNet Developer Job'\n",
            " 'Generate a Resume for a ETL Developer Job'\n",
            " 'Generate a Resume for a Information Technology Job'\n",
            " 'Generate a Resume for a Java Developer Job'\n",
            " 'Generate a Resume for a Python Developer Job'\n",
            " 'Generate a Resume for a SAP Developer Job'\n",
            " 'Generate a Resume for a Software Developer Job'\n",
            " 'Generate a Resume for a Web Designing Job'\n",
            " 'Generate a Resume for a Web Developer Job']\n"
          ]
        }
      ],
      "source": [
        "# filter the resume by IT-related resumes only due to high number of resumes\n",
        "filtered_jobs_df1 = [\n",
        "    \"data science\",\n",
        "    \"database\",\n",
        "    \"devops\",\n",
        "    \"developer\",\n",
        "    \"information technology\",\n",
        "    \"web\"\n",
        "]\n",
        "filtered_df1 = df1[df1['instruction'].str.contains('|'.join(filtered_jobs_df1), case=False, regex=True)]\n",
        "print(\"Distinct values in 'instruction' column of filtered_df:\", filtered_df1['instruction'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a53042",
      "metadata": {
        "id": "89a53042",
        "outputId": "50ecd885-bd2f-4b7e-ce6d-0a7afcf72b42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 17324 entries, 1428 to 32480\n",
            "Data columns (total 1 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   Resume_test  17324 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 270.7+ KB\n"
          ]
        }
      ],
      "source": [
        "# drop columns\n",
        "df1 = filtered_df1.drop(columns=['instruction', 'input'])\n",
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a8af33",
      "metadata": {
        "id": "e6a8af33"
      },
      "outputs": [],
      "source": [
        "df1.to_csv('/content/drive/MyDrive/FYP/Implementation/Resume Dataset/resume_collection_1.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess df6"
      ],
      "metadata": {
        "id": "vvhBGi1rqpym"
      },
      "id": "vvhBGi1rqpym"
    },
    {
      "cell_type": "code",
      "source": [
        "df6.info()"
      ],
      "metadata": {
        "id": "2DC-1G0yqYZl",
        "outputId": "4cfd3bfb-679d-4c07-c746-6ea087964f2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2DC-1G0yqYZl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13389 entries, 0 to 13388\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Category  13389 non-null  object\n",
            " 1   Text      13389 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 209.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print distinct category in df6\n",
        "distinct_category_df6 = df6['Category'].unique()\n",
        "print(\"Distinct values in 'category' column in df6:\", distinct_category_df6)"
      ],
      "metadata": {
        "id": "IiARNQQWqqI2",
        "outputId": "7d05de26-6ba3-407d-c7dd-d3e6d958b020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IiARNQQWqqI2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct values in 'category' column in df6: ['Accountant' 'Advocate' 'Agriculture' 'Apparel' 'Architecture' 'Arts'\n",
            " 'Automobile' 'Aviation' 'Banking' 'Blockchain' 'BPO'\n",
            " 'Building and Construction' 'Business Analyst' 'Civil Engineer'\n",
            " 'Consultant' 'Data Science' 'Database' 'Designing' 'DevOps'\n",
            " 'Digital Media' 'DotNet Developer' 'Education' 'Electrical Engineering'\n",
            " 'ETL Developer' 'Finance' 'Food and Beverages' 'Health and Fitness'\n",
            " 'Human Resources' 'Information Technology' 'Java Developer' 'Management'\n",
            " 'Mechanical Engineer' 'Network Security Engineer' 'Operations Manager'\n",
            " 'PMO' 'Public Relations' 'Python Developer' 'React Developer' 'Sales'\n",
            " 'SAP Developer' 'SQL Developer' 'Testing' 'Web Designing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export rows that didnt include deplicated rows in previously exported files\n",
        "# for IT-related jobs\n",
        "\n",
        "import pandas as pd\n",
        "import os, glob\n",
        "\n",
        "drive_folder_path = '/content/drive/MyDrive/FYP/Implementation/Resume Dataset/'\n",
        "\n",
        "# load all previous files matching the naming pattern in the specified folder\n",
        "existing_files_A = glob.glob(os.path.join(drive_folder_path, \"resume_collection_6_a_*.csv\"))\n",
        "file_count_A = len(existing_files_A)\n",
        "\n",
        "# merge horizontally all existing files\n",
        "existing_data_A = pd.concat([pd.read_csv(f) for f in existing_files_A])\n",
        "\n",
        "# filter out rows that have already been exported\n",
        "new_data_df_A = df6[~df6['Text'].isin(existing_data_A['Text'])]\n",
        "\n",
        "# further filter the data to include only the specified job categories (IT-related)\n",
        "filtered_jobs_df6_A = [\n",
        "    \"data science\",\n",
        "    \"database\",\n",
        "    \"devops\",\n",
        "    \"developer\",\n",
        "    \"information technology\"\n",
        "]\n",
        "filtered_new_data_df_A = new_data_df_A[new_data_df_A['Category'].str.contains('|'.join(filtered_jobs_df6_A), case=False, regex=True)]\n",
        "\n",
        "# shuffle and sample 200 rows\n",
        "sampled_df_A = filtered_new_data_df_A.sample(n=200, random_state=42)\n",
        "\n",
        "# display the distinct categories in the new sample\n",
        "print(\"Distinct values in 'Category' column in sampled_df:\", sampled_df_A['Category'].unique())\n",
        "\n",
        "# export the new sample to a CSV\n",
        "new_file_path_A = os.path.join(drive_folder_path, f\"resume_collection_6_a_{file_count_A + 1}.csv\")\n",
        "sampled_df_A.to_csv(new_file_path_A, index=False)\n",
        "print(\"Save to\", new_file_path_A)\n"
      ],
      "metadata": {
        "id": "eQVcEDR0qUHL",
        "outputId": "b6a73638-2b4b-46b5-c273-56db47062a14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eQVcEDR0qUHL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct values in 'Category' column in sampled_df: ['DevOps' 'ETL Developer' 'SAP Developer' 'DotNet Developer'\n",
            " 'Data Science' 'SQL Developer' 'Database' 'Python Developer'\n",
            " 'Information Technology' 'Java Developer' 'React Developer']\n",
            "Save to /content/drive/MyDrive/FYP/Implementation/Resume Dataset/resume_collection_6_a_2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export rows that didnt include deplicated rows in previously exported files\n",
        "# for IT-related jobs\n",
        "\n",
        "import pandas as pd\n",
        "import os, glob\n",
        "\n",
        "drive_folder_path = '/content/drive/MyDrive/FYP/Implementation/Resume Dataset/'\n",
        "\n",
        "# load all previous files matching the naming pattern in the specified folder\n",
        "existing_files_B = glob.glob(os.path.join(drive_folder_path, \"resume_collection_6_b_*.csv\"))\n",
        "file_count_B = len(existing_files_B)\n",
        "\n",
        "# merge horizontally all existing files\n",
        "existing_data_B = pd.concat([pd.read_csv(f) for f in existing_files_B])\n",
        "\n",
        "# filter out rows that have already been exported\n",
        "new_data_df_B = df6[~df6['Text'].isin(existing_data_B['Text'])]\n",
        "\n",
        "# further filter the data to include only the specified job categories (non IT-related)\n",
        "filtered_jobs_df6_A = [\n",
        "    \"data science\",\n",
        "    \"database\",\n",
        "    \"devops\",\n",
        "    \"developer\",\n",
        "    \"information technology\"\n",
        "]\n",
        "filtered_new_data_df_B = new_data_df_B[~new_data_df_B['Category'].str.contains('|'.join(filtered_jobs_df6_A), case=False, regex=True)]\n",
        "\n",
        "# shuffle and sample 200 rows\n",
        "sampled_df_B = filtered_new_data_df_B.sample(n=200, random_state=42)\n",
        "\n",
        "# display the distinct categories in the new sample\n",
        "print(\"Distinct values in 'Category' column in sampled_df:\", sampled_df_B['Category'].unique())\n",
        "\n",
        "# export the new sample to a CSV\n",
        "new_file_path_B = os.path.join(drive_folder_path, f\"resume_collection_6_b_{file_count_B + 1}.csv\")\n",
        "sampled_df_B.to_csv(new_file_path_B, index=False)\n",
        "print(\"Save to\", new_file_path_B)"
      ],
      "metadata": {
        "id": "UYcyJvUpwFZD",
        "outputId": "78608f8f-b7f8-492b-e9e2-de3ba1510083",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UYcyJvUpwFZD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distinct values in 'Category' column in sampled_df: ['Civil Engineer' 'Health and Fitness' 'Digital Media' 'Web Designing'\n",
            " 'Building and Construction' 'Electrical Engineering'\n",
            " 'Network Security Engineer' 'Accountant' 'Aviation' 'Business Analyst'\n",
            " 'Education' 'Public Relations' 'Finance' 'PMO' 'Advocate' 'Designing'\n",
            " 'Apparel' 'Arts' 'Testing' 'Automobile' 'Consultant' 'Architecture'\n",
            " 'Human Resources' 'Agriculture' 'Sales' 'BPO' 'Operations Manager'\n",
            " 'Management' 'Mechanical Engineer' 'Banking' 'Food and Beverages']\n",
            "Save to /content/drive/MyDrive/FYP/Implementation/Resume Dataset/resume_collection_6_b_2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Data to Improve Performance on NAME, LOC, EMAIL, PHONE Labels\n",
        "**Problem Statement**\n",
        "1. In the resume dataset, many **NAME**, **EMAIL**, and **PHONE** entities are censored and replaced with same or similar dummy values, such as *jessica claire* (NAME) and *resumesampleexamplecom* (EMAIL).\n",
        "2. In the resume dataset, **NAME** and **LOC** entities are limited to American names and locations only.\n",
        "\n",
        "As a result, although metrics like precision and F1-score for these classes are relatively high compared to other classes, the trained models struggle to correctly identify these entities when tested on real resumes.\n",
        "\n",
        "**Solution**\n",
        "\n",
        "1. Prepare another dataset which contains more variety of NAME, EMAIL, PHONE, LOC entities values.\n",
        "2. Split into train and test set and combine with the train and test test of resume dataset respectively.\n",
        "\n",
        "**Dataset**\n",
        "1. https://huggingface.co/datasets/Josephgflowers/PII-NER (NAME, EMAIL, PHONE. Note: only indian names)\n",
        "2. https://huggingface.co/datasets/Josephgflowers/CENSUS-NER-Name-Email-Address-Phone (NAME, EMAIL, PHONE)"
      ],
      "metadata": {
        "id": "PQ42XgxRlg6S"
      },
      "id": "PQ42XgxRlg6S"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WV7O4ra1lgMa"
      },
      "id": "WV7O4ra1lgMa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KFG5IuORlgeg"
      },
      "id": "KFG5IuORlgeg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}