{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chewzzz1014/fyp/blob/master/ner/src/train_ner_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train NER Models"
      ],
      "metadata": {
        "id": "t0P9v82yDulr"
      },
      "id": "t0P9v82yDulr"
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5YToyjV-40ZV",
        "outputId": "2621d6ac-2728-45b0-9b45-7ac34860ee3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "5YToyjV-40ZV",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir spacy_ner_data"
      ],
      "metadata": {
        "id": "VtZ7OFQtpG4-"
      },
      "id": "VtZ7OFQtpG4-",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "\n",
        "# Load JSON data\n",
        "with open('/content/drive/MyDrive/FYP/Implementation/Resume Dataset/200_resumes_annotated.json', \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def remove_overlapping_entities(entities):\n",
        "    \"\"\"Remove overlapping entities from the list.\"\"\"\n",
        "    entities = sorted(entities, key=lambda x: x[0])  # Sort by start position\n",
        "    non_overlapping = []\n",
        "    last_end = -1\n",
        "    for start, end, label in entities:\n",
        "        if start >= last_end:  # Only add if there's no overlap with the previous entity\n",
        "            non_overlapping.append((start, end, label))\n",
        "            last_end = end\n",
        "    return non_overlapping\n",
        "\n",
        "# Function to convert JSON data to Spacy's DocBin format\n",
        "def convert_to_spacy_format(data):\n",
        "    nlp = spacy.blank(\"en\")  # Load a blank Spacy model\n",
        "    doc_bin = DocBin()  # Container for our docs\n",
        "\n",
        "    for item in data:\n",
        "        text = item['data']['Text']  # Full document text\n",
        "        entities = []\n",
        "\n",
        "        for annotation in item['annotations'][0]['result']:\n",
        "            start = annotation['value']['start']\n",
        "            end = annotation['value']['end']\n",
        "            label = annotation['value']['labels'][0]  # Entity label\n",
        "            entities.append((start, end, label))\n",
        "\n",
        "        entities = remove_overlapping_entities(entities)  # Remove overlapping entities\n",
        "        # Create a Spacy doc and add entities to it\n",
        "        doc = nlp.make_doc(text)\n",
        "        spans = [doc.char_span(start, end, label=label) for start, end, label in entities]\n",
        "        # Filter out None spans if Spacy can't align the character indices with tokens\n",
        "        spans = [span for span in spans if span is not None]\n",
        "        doc.ents = spans  # Assign entities to the doc\n",
        "        doc_bin.add(doc)\n",
        "\n",
        "    return doc_bin\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert train and test sets to Spacy format\n",
        "train_doc_bin = convert_to_spacy_format(train_data)\n",
        "test_doc_bin = convert_to_spacy_format(test_data)\n",
        "\n",
        "# Save the train and test data to .spacy files\n",
        "train_doc_bin.to_disk(\"spacy_ner_data/train_data.spacy\")\n",
        "test_doc_bin.to_disk(\"spacy_ner_data/test_data.spacy\")"
      ],
      "metadata": {
        "id": "R-8QBDX3gtkp"
      },
      "id": "R-8QBDX3gtkp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spacy NER"
      ],
      "metadata": {
        "id": "FMM7E6hNCEUv"
      },
      "id": "FMM7E6hNCEUv"
    },
    {
      "cell_type": "code",
      "source": [
        "# create base_config.cfg and paste the config generated from spacy widget\n",
        "# update train and test file path\n",
        "!touch base_config.cfg"
      ],
      "metadata": {
        "id": "ruPln43fCDLm"
      },
      "id": "ruPln43fCDLm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate config.cfg from base_config.cfg\n",
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ],
      "metadata": {
        "id": "0aRjL9jqC-5k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5489cd43-f730-47d0-ca13-c1ed658d98da"
      },
      "id": "0aRjL9jqC-5k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wb_aBw9X2NU",
        "outputId": "561939c7-f8b1-4038-af0d-ddd3824b3509"
      },
      "id": "9wb_aBw9X2NU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train model using hyperparameters set in config.cfg\n",
        "# save trained model in spacy-output/ dir\n",
        "!python -m spacy train config.cfg --output ./spacy_output\n",
        "!cp -r ./spacy_output /content/drive/MyDrive/FYP/Implementation/"
      ],
      "metadata": {
        "id": "Jav_l46BEDjF"
      },
      "id": "Jav_l46BEDjF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate trained model performance\n",
        "# store output and visualization into result/ dir\n",
        "!python -m spacy evaluate spacy_output/model-best spacy_ner_data/test_data.spacy -dp spacy_output"
      ],
      "metadata": {
        "id": "V2wzIOsWJgkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f701bb-ffdb-4167-f67a-0146a54859ba"
      },
      "id": "V2wzIOsWJgkG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "TOK     100.00\n",
            "NER P   51.12 \n",
            "NER R   41.26 \n",
            "NER F   45.66 \n",
            "SPEED   2395  \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                P       R       F\n",
            "NAME        89.66   78.79   83.87\n",
            "JOB         72.00   32.43   44.72\n",
            "DEG         62.16   63.89   63.01\n",
            "UNI         38.89   34.15   36.36\n",
            "EMAIL       63.33   95.00   76.00\n",
            "LOC         39.39   31.71   35.14\n",
            "WORK PER    75.45   83.00   79.05\n",
            "COMPANY     28.42   36.49   31.95\n",
            "SKILL       40.96   28.96   33.93\n",
            "PHONE       89.66   83.87   86.67\n",
            "STUDY PER   65.62   58.33   61.76\n",
            "\n",
            "<IPython.core.display.HTML object>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/cli/_util.py\", line 87, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 728, in main\n",
            "    return _main(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typer/core.py\", line 197, in _main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/typer/main.py\", line 703, in wrapper\n",
            "    return callback(**use_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/cli/evaluate.py\", line 47, in evaluate_cli\n",
            "    evaluate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/cli/evaluate.py\", line 141, in evaluate\n",
            "    render_parses(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/cli/evaluate.py\", line 208, in render_parses\n",
            "    file_.write(html)\n",
            "TypeError: write() argument must be str, not None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make prediction\n",
        "import spacy\n",
        "resume_text = '''\n",
        "John Doe lives at 1234 Elm Street in Los Angeles, CA 90001. He can be reached at +1 (555) 123-4567 or via email at john.doe@example.com. John is a results-driven software engineer with over 5 years of experience in web development and cloud infrastructure, with strong knowledge of JavaScript, Python, and cloud technologies like AWS and Azure. Currently, he works as a Software Engineer at Google LLC in San Francisco, CA, where he has been employed since August 2019. In this role, he has developed scalable web applications using JavaScript, Node.js, and React, deployed and maintained cloud infrastructure on AWS, reducing downtime by 20%, and led a team of 4 engineers to enhance backend performance by 30%. Previously, he worked as a Junior Developer at Tech Innovators Inc. in Austin, TX, from July 2017 to July 2019, where he created RESTful APIs using Python and Flask, collaborated with front-end developers to build and deploy user-facing applications, and wrote unit and integration tests, improving code coverage by 15%.\n",
        "\n",
        "John holds a Master of Science in Computer Science from the University of California, Berkeley, with a graduation date of May 2017, and a Bachelor of Science in Information Technology from the University of Texas at Austin, graduated in May 2015. His skillset includes proficiency in programming languages like Python, JavaScript, and Java; frameworks such as React, Flask, and Django; cloud platforms including AWS, Google Cloud, and Azure; as well as other tools like Git, Docker, Kubernetes, and SQL. He is certified as an AWS Certified Solutions Architect – Associate, earned in 2020, and as a Google Professional Cloud Architect, earned in 2021'\n",
        "'''\n",
        "nlp = spacy.load(\"spacy-output/model-best\")\n",
        "doc = nlp(resume_text.lower())\n",
        "\n",
        "print(doc.ents)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text}: {ent.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXAlZZkEeeMc",
        "outputId": "b8cb33e8-de59-4fb2-9c8e-b54180992c00"
      },
      "id": "UXAlZZkEeeMc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(john doe, in los, (555) 123-4567, john.doe@example.com, john is, aws, restful apis, master of science, bachelor of science in information technology, python, aws, azure, git, docker)\n",
            "john doe: NAME\n",
            "in los: LOC\n",
            "(555) 123-4567: PHONE\n",
            "john.doe@example.com: EMAIL\n",
            "john is: NAME\n",
            "aws: SKILL\n",
            "restful apis: SKILL\n",
            "master of science: DEG\n",
            "bachelor of science in information technology: DEG\n",
            "python: SKILL\n",
            "aws: SKILL\n",
            "azure: SKILL\n",
            "git: SKILL\n",
            "docker: SKILL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "LtxtV7GufiOw",
        "outputId": "670daab7-d8e9-4f21-f231-84c81cb6f23d"
      },
      "id": "LtxtV7GufiOw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    john doe\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME</span>\n",
              "</mark>\n",
              " lives at 1234 elm street \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    in los\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " angeles, ca 90001. he can be reached at +1 \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    (555) 123-4567\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PHONE</span>\n",
              "</mark>\n",
              " or via email at \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    john.doe@example.com\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">EMAIL</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    john is\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NAME</span>\n",
              "</mark>\n",
              " a results-driven software engineer with over 5 years of experience in web development and cloud infrastructure, with strong knowledge of javascript, python, and cloud technologies like \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    aws\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
              "</mark>\n",
              " and azure. currently, he works as a software engineer at google llc in san francisco, ca, where he has been employed since august 2019. in this role, he has developed scalable web applications using javascript, node.js, and react, deployed and maintained cloud infrastructure on aws, reducing downtime by 20%, and led a team of 4 engineers to enhance backend performance by 30%. previously, he worked as a junior developer at tech innovators inc. in austin, tx, from july 2017 to july 2019, where he created \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    restful apis\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
              "</mark>\n",
              " using python and flask, collaborated with front-end developers to build and deploy user-facing applications, and wrote unit and integration tests, improving code coverage by 15%.<br><br>john holds a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    master of science\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DEG</span>\n",
              "</mark>\n",
              " in computer science from the university of california, berkeley, with a graduation date of may 2017, and a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    bachelor of science in information technology\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DEG</span>\n",
              "</mark>\n",
              " from the university of texas at austin, graduated in may 2015. his skillset includes proficiency in programming languages like \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    python\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
              "</mark>\n",
              ", javascript, and java; frameworks such as react, flask, and django; cloud platforms including \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    aws\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
              "</mark>\n",
              ", google cloud, and \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    azure\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
              "</mark>\n",
              "; as well as other tools like \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    git\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    docker\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">SKILL</span>\n",
              "</mark>\n",
              ", kubernetes, and sql. he is certified as an aws certified solutions architect – associate, earned in 2020, and as a google professional cloud architect, earned in 2021'<br></div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flair NER"
      ],
      "metadata": {
        "id": "1z_niSalE0uT"
      },
      "id": "1z_niSalE0uT"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair"
      ],
      "metadata": {
        "id": "lsEUGnPpnjuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f032982a-4586-4c85-b34d-c054fd3aa9eb"
      },
      "id": "lsEUGnPpnjuU",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flair\n",
            "  Downloading flair-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting boto3>=1.20.27 (from flair)\n",
            "  Downloading boto3-1.35.54-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting conllu<5.0.0,>=4.0 (from flair)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.14)\n",
            "Collecting ftfy>=6.1.0 (from flair)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair) (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from flair) (0.24.7)\n",
            "Collecting langdetect>=1.0.9 (from flair)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair) (5.3.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (3.8.0)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.10/dist-packages (from flair) (10.5.0)\n",
            "Collecting mpld3>=0.3 (from flair)\n",
            "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pptree>=3.1 (from flair)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from flair) (2.8.2)\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from flair) (2024.9.11)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair) (1.5.2)\n",
            "Collecting segtok>=1.5.11 (from flair)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.66.6)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
            "  Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (4.44.2)\n",
            "Collecting wikipedia-api>=0.5.7 (from flair)\n",
            "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting semver<4.0.0,>=3.0.0 (from flair)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting bioc<3.0.0,>=2.0.0 (from flair)\n",
            "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.36.0,>=1.35.54 (from boto3>=1.20.27->flair)\n",
            "  Downloading botocore-1.35.54-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair)\n",
            "  Downloading s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair) (1.16.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (2.32.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect>=1.0.9->flair) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.26.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (3.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mpld3>=0.3->flair) (3.1.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (3.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.18.0->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.18.0->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.19.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.2.0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.54->boto3>=1.20.27->flair) (2.2.3)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (24.2.0)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (0.34.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.6)\n",
            "Collecting sortedcontainers<3.0,>=2.0 (from intervaltree->bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mpld3>=0.3->flair) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.5)\n",
            "Downloading flair-0.14.0-py3-none-any.whl (776 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.5/776.5 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bioc-2.1-py3-none-any.whl (33 kB)\n",
            "Downloading boto3-1.35.54-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading botocore-1.35.54-py3-none-any.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: langdetect, pptree, sqlitedict, wikipedia-api, docopt, intervaltree\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=35bf87f8bf26a6f99706609899159a7150cc4ff575e7ca93b37e17cce81b5b63\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=682eb9733826b92cd4f9888ff3f5a4f43f1e9ac70cb8aa8ccc611e94c9fb5708\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=c4cbdd46cbc3557ce25b9239b3406daa0dcee900b994687e07abd6d898b0cc84\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=7df1b8daf74d63a494517210f3c59685b85eb40e409a64369a6976ef97ad56db\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=79b9720a3fae88e9f3922521bb0530ea71aee765c14010218b9403f8d1c4c907\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=2f7afdf8857da0f66798988cba209befc994fc8fba22632d6db864d0d71affaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/80/8c/43488a924a046b733b64de3fac99252674c892a4c3801c0a61\n",
            "Successfully built langdetect pptree sqlitedict wikipedia-api docopt intervaltree\n",
            "Installing collected packages: sqlitedict, sortedcontainers, pptree, docopt, semver, segtok, langdetect, jsonlines, jmespath, intervaltree, ftfy, conllu, wikipedia-api, botocore, bioc, s3transfer, pytorch-revgrad, mpld3, boto3, transformer-smaller-training-vocab, flair\n",
            "Successfully installed bioc-2.1 boto3-1.35.54 botocore-1.35.54 conllu-4.5.3 docopt-0.6.2 flair-0.14.0 ftfy-6.3.1 intervaltree-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 mpld3-0.5.10 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.10.3 segtok-1.5.11 semver-3.0.2 sortedcontainers-2.4.0 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.4.0 wikipedia-api-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert spacy data into flair data\n",
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "import os\n",
        "\n",
        "def convert_spacy_to_flair(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Convert SpaCy binary format to Flair's CoNLL format.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to SpaCy binary file (.spacy)\n",
        "        output_file (str): Path to output file for Flair format\n",
        "    \"\"\"\n",
        "    # Load spaCy model\n",
        "    nlp = spacy.blank(\"en\")\n",
        "\n",
        "    # Load the DocBin\n",
        "    doc_bin = DocBin().from_disk(input_file)\n",
        "    docs = list(doc_bin.get_docs(nlp.vocab))\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for doc in docs:\n",
        "            tokens = [(t.text, t.ent_iob_, t.ent_type_) for t in doc]\n",
        "\n",
        "            # Write tokens in CoNLL format\n",
        "            for token in tokens:\n",
        "                text, iob, ent_type = token\n",
        "\n",
        "                # Convert spaCy IOB to CoNLL format\n",
        "                if iob == 'O':\n",
        "                    tag = 'O'\n",
        "                else:\n",
        "                    tag = f'{iob}-{ent_type}' if ent_type else 'O'\n",
        "\n",
        "                # Write line: token and NER tag\n",
        "                f.write(f'{text} {tag}\\n')\n",
        "\n",
        "            # Empty line between sentences\n",
        "            f.write('\\n')\n",
        "\n",
        "def convert_spacy_json_to_flair(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Convert SpaCy JSON format to Flair's CoNLL format.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to JSON file with SpaCy annotations\n",
        "        output_file (str): Path to output file for Flair format\n",
        "    \"\"\"\n",
        "    import json\n",
        "\n",
        "    nlp = spacy.blank(\"en\")\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        training_data = json.load(f)\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for example in training_data:\n",
        "            text = example['text']\n",
        "            ents = example.get('entities', [])\n",
        "\n",
        "            # Create a spaCy doc\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Add entities to doc\n",
        "            spans = []\n",
        "            for start, end, label in ents:\n",
        "                span = doc.char_span(start, end, label=label)\n",
        "                if span is not None:\n",
        "                    spans.append(span)\n",
        "            doc.ents = spans\n",
        "\n",
        "            # Convert to CoNLL format\n",
        "            tokens = [(t.text, t.ent_iob_, t.ent_type_) for t in doc]\n",
        "\n",
        "            for token in tokens:\n",
        "                text, iob, ent_type = token\n",
        "                if iob == 'O':\n",
        "                    tag = 'O'\n",
        "                else:\n",
        "                    tag = f'{iob}-{ent_type}' if ent_type else 'O'\n",
        "                f.write(f'{text} {tag}\\n')\n",
        "\n",
        "            f.write('\\n')\n",
        "\n",
        "# Example usage for JSON format\n",
        "flair_train_json = \"flair_train.txt\"\n",
        "flair_test_json = \"flair_test.txt\"\n",
        "\n",
        "convert_spacy_to_flair('/content/spacy_ner_data/train_data.spacy', flair_train_json)\n",
        "convert_spacy_to_flair('/content/spacy_ner_data/test_data.spacy', flair_test_json)"
      ],
      "metadata": {
        "id": "8SgkTXeIiTtF"
      },
      "id": "8SgkTXeIiTtF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert spacy data into flair data\n",
        "import spacy\n",
        "from spacy.training import Corpus\n",
        "\n",
        "!python -m spacy download de_core_news_sm\n",
        "nlp = spacy.load(\"de_core_news_sm\")\n",
        "corpus = Corpus(\"/content/spacy_ner_data/test_data.spacy\")\n",
        "\n",
        "data = corpus(nlp)\n",
        "\n",
        "# Flair supports BIO and BIOES, see https://github.com/flairNLP/flair/issues/875\n",
        "def rename_biluo_to_bioes(old_tag):\n",
        "    new_tag = \"\"\n",
        "    try:\n",
        "        if old_tag.startswith(\"L\"):\n",
        "            new_tag = \"E\" + old_tag[1:]\n",
        "        elif old_tag.startswith(\"U\"):\n",
        "            new_tag = \"S\" + old_tag[1:]\n",
        "        else:\n",
        "            new_tag = old_tag\n",
        "    except:\n",
        "        pass\n",
        "    return new_tag\n",
        "\n",
        "\n",
        "def generate_corpus():\n",
        "    corpus = []\n",
        "    n_ex = 0\n",
        "    for example in data:\n",
        "        n_ex += 1\n",
        "        text = example.text\n",
        "        doc = nlp(text)\n",
        "        tags = example.get_aligned_ner()\n",
        "        # Check if it's an empty list of NER tags.\n",
        "        if None in tags:\n",
        "            pass\n",
        "        else:\n",
        "            new_tags = [rename_biluo_to_bioes(tag) for tag in tags]\n",
        "            for token, tag in zip(doc,new_tags):\n",
        "                row = token.text +' '+ token.pos_ +' ' +tag + '\\n'\n",
        "                corpus.append(row)\n",
        "            corpus.append('\\n')\n",
        "    return corpus\n",
        "\n",
        "def write_file(filepath):\n",
        "    with open(filepath, 'w', encoding='utf-8') as f:\n",
        "        corpus = generate_corpus()\n",
        "        f.writelines(corpus)\n",
        "\n",
        "def main():\n",
        "    write_file('flair_test.txt')\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "TbHBmnUyv65f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be6d42bd-f6a2-4143-b278-f010c011ae8f"
      },
      "id": "TbHBmnUyv65f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting de-core-news-sm==3.7.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert json into flair data\n",
        "import json\n",
        "import random\n",
        "from typing import List, Dict, Tuple\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "\n",
        "class NERConverter:\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def get_bioes_label(self, token_index: int, entity_length: int, current_position: int, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Convert to BIOES format\n",
        "        - S-: Single token entity\n",
        "        - B-: Beginning of multi-token entity\n",
        "        - I-: Inside of multi-token entity\n",
        "        - E-: End of multi-token entity\n",
        "        - O: Outside\n",
        "        \"\"\"\n",
        "        if entity_length == 1:\n",
        "            return f'S-{label}'\n",
        "        if current_position == 0:\n",
        "            return f'B-{label}'\n",
        "        if current_position == entity_length - 1:\n",
        "            return f'E-{label}'\n",
        "        return f'I-{label}'\n",
        "\n",
        "    def convert_to_bioes_format(self, json_data: List[dict]) -> List[List[Tuple[str, str]]]:\n",
        "        \"\"\"Convert JSON annotations to BIOES format.\"\"\"\n",
        "        all_sentences = []\n",
        "\n",
        "        for item in json_data:\n",
        "            text = item['data']['Text']\n",
        "            doc = self.nlp(text)\n",
        "\n",
        "            # Initialize character-level labels\n",
        "            char_labels = ['O'] * len(text)\n",
        "\n",
        "            # First pass: identify entity boundaries and lengths\n",
        "            entity_spans = []\n",
        "            if item['annotations'] and len(item['annotations']) > 0:\n",
        "                for ann in item['annotations'][0]['result']:\n",
        "                    if 'value' in ann:\n",
        "                        start = ann['value']['start']\n",
        "                        end = ann['value']['end']\n",
        "                        label = ann['value']['labels'][0]\n",
        "                        entity_spans.append((start, end, label))\n",
        "\n",
        "            # Sort spans by start position\n",
        "            entity_spans.sort(key=lambda x: x[0])\n",
        "\n",
        "            # Second pass: apply BIOES labels\n",
        "            for start, end, label in entity_spans:\n",
        "                # Get tokens that are part of this entity\n",
        "                entity_text = text[start:end]\n",
        "                entity_doc = self.nlp(entity_text)\n",
        "                entity_length = len([token for token in entity_doc if not token.is_space])\n",
        "\n",
        "                # Set labels for the entire span\n",
        "                current_token_idx = 0\n",
        "                for i in range(start, end):\n",
        "                    if i == start or text[i-1].isspace():\n",
        "                        char_labels[i] = self.get_bioes_label(i, entity_length, current_token_idx, label)\n",
        "                        current_token_idx += 1\n",
        "                    else:\n",
        "                        char_labels[i] = char_labels[i-1]\n",
        "\n",
        "            # Convert to token-level labels\n",
        "            current_sentence = []\n",
        "            for sent in doc.sents:\n",
        "                for token in sent:\n",
        "                    # Get the most common label for the token's characters\n",
        "                    token_chars_labels = char_labels[token.idx:token.idx + len(token.text)]\n",
        "                    label_counts = defaultdict(int)\n",
        "                    for char_label in token_chars_labels:\n",
        "                        label_counts[char_label] += 1\n",
        "\n",
        "                    token_label = max(label_counts.items(), key=lambda x: x[1])[0]\n",
        "                    current_sentence.append((token.text, token_label))\n",
        "\n",
        "                if current_sentence:\n",
        "                    all_sentences.append(current_sentence)\n",
        "                    current_sentence = []\n",
        "\n",
        "        return all_sentences\n",
        "\n",
        "    def write_flair_file(self, sentences: List[List[Tuple[str, str]]], filename: str):\n",
        "        \"\"\"Write sentences in BIOES format to file.\"\"\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            for sentence in sentences:\n",
        "                for token, label in sentence:\n",
        "                    f.write(f'{token} {label}\\n')\n",
        "                f.write('\\n')\n",
        "\n",
        "    def convert_and_split(self, json_data: List[dict], train_file: str, test_file: str, test_ratio: float = 0.2):\n",
        "        \"\"\"Convert JSON to BIOES format and split into train/test sets.\"\"\"\n",
        "        all_sentences = self.convert_to_bioes_format(json_data)\n",
        "\n",
        "        # Shuffle and split\n",
        "        random.shuffle(all_sentences)\n",
        "        split_idx = int(len(all_sentences) * (1 - test_ratio))\n",
        "\n",
        "        train_sentences = all_sentences[:split_idx]\n",
        "        test_sentences = all_sentences[split_idx:]\n",
        "\n",
        "        # Write to files\n",
        "        self.write_flair_file(train_sentences, train_file)\n",
        "        self.write_flair_file(test_sentences, test_file)\n",
        "\n",
        "        return len(train_sentences), len(test_sentences)\n",
        "\n",
        "def main():\n",
        "    # Load JSON data\n",
        "    with open('/content/drive/MyDrive/FYP/Implementation/Resume Dataset/200_resumes_annotated.json', 'r', encoding='utf-8') as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    # Convert and split data\n",
        "    converter = NERConverter()\n",
        "    train_count, test_count = converter.convert_and_split(\n",
        "        json_data,\n",
        "        train_file='flair_train.txt',\n",
        "        test_file='flair_test.txt',\n",
        "        test_ratio=0.2\n",
        "    )\n",
        "\n",
        "    print(f'Created {train_count} training sentences and {test_count} test sentences')\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYAkAghgs9k5",
        "outputId": "bd42166d-9c25-464c-d1a9-cb936acbbc4e"
      },
      "id": "JYAkAghgs9k5",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 298 training sentences and 75 test sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "\n",
        "# Define columns for CoNLL (0: word, 1: label)\n",
        "columns = {0: 'text', 1: 'ner'}\n",
        "\n",
        "# Set data folder and file names\n",
        "data_folder = './'\n",
        "train_file = 'flair_train.txt'\n",
        "test_file = 'flair_test.txt'\n",
        "\n",
        "# Load the corpus\n",
        "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
        "                              train_file=train_file,\n",
        "                              test_file=test_file,\n",
        "                              dev_file=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl2-GpGBjpsi",
        "outputId": "f27b6863-1c15-4c93-feb3-8c5aa7e5996c"
      },
      "id": "vl2-GpGBjpsi",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 08:49:53,093 Reading data from .\n",
            "2024-11-05 08:49:53,096 Train: flair_train.txt\n",
            "2024-11-05 08:49:53,099 Dev: None\n",
            "2024-11-05 08:49:53,103 Test: flair_test.txt\n",
            "2024-11-05 08:49:55,631 No dev split found. Using 10% (i.e. 30 samples) of the train split as dev data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag_dictionary = corpus.make_label_dictionary(label_type='ner')\n",
        "print(\"Labels:\", tag_dictionary.get_items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_skLRc8i44m",
        "outputId": "cc3c5ddc-b5e9-4d9e-b936-06d98d593148"
      },
      "id": "d_skLRc8i44m",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 08:50:02,511 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n",
            "268it [00:00, 9729.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 08:50:02,648 Dictionary created for label 'ner' with 11 values: SKILL (seen 2101 times), JOB (seen 546 times), WORK (seen 463 times), COMPANY (seen 368 times), LOC (seen 249 times), UNI (seen 155 times), DEG (seen 152 times), NAME (seen 147 times), PHONE (seen 144 times), STUDY (seen 134 times), EMAIL (seen 115 times)\n",
            "Labels: ['SKILL', 'JOB', 'WORK', 'COMPANY', 'LOC', 'UNI', 'DEG', 'NAME', 'PHONE', 'STUDY', 'EMAIL']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def count_labels(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        labels = [line.split()[-1] for line in file if line.strip()]\n",
        "    return Counter(labels)\n",
        "\n",
        "print(\"Train label distribution:\", count_labels('flair_train.txt'))\n",
        "print(\"Test label distribution:\", count_labels('flair_test.txt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5cXWOiGY5an",
        "outputId": "cd2fb479-3b7a-48b1-9d46-8263c8ff0fe6"
      },
      "id": "j5cXWOiGY5an",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train label distribution: Counter({'O': 73280, 'S-SKILL': 1447, 'PER': 1223, 'B-SKILL': 993, 'E-SKILL': 991, 'E-JOB': 541, 'B-JOB': 532, 'E-COMPANY': 341, 'B-COMPANY': 336, 'I-JOB': 315, 'I-DEG': 253, 'I-COMPANY': 235, 'I-SKILL': 200, 'S-LOC': 170, 'E-UNI': 170, 'B-UNI': 169, 'B-NAME': 163, 'E-NAME': 163, 'E-DEG': 157, 'B-DEG': 156, 'I-UNI': 136, 'E-PHONE': 131, 'B-PHONE': 130, 'S-EMAIL': 124, 'B-LOC': 106, 'E-LOC': 106, 'I-PHONE': 91, 'S-COMPANY': 66, 'S-JOB': 64, 'S-PHONE': 25, 'I-NAME': 10, 'S-DEG': 7, 'I-LOC': 5, 'S-UNI': 2, 'B-EMAIL': 1, 'E-EMAIL': 1})\n",
            "Test label distribution: Counter({'O': 15813, 'S-SKILL': 344, 'PER': 288, 'B-SKILL': 220, 'E-SKILL': 218, 'E-JOB': 118, 'B-JOB': 115, 'B-COMPANY': 86, 'E-COMPANY': 86, 'I-DEG': 81, 'I-COMPANY': 80, 'B-DEG': 45, 'E-DEG': 45, 'B-UNI': 45, 'E-UNI': 45, 'I-JOB': 42, 'I-SKILL': 36, 'S-LOC': 35, 'I-UNI': 35, 'B-NAME': 28, 'E-NAME': 28, 'E-PHONE': 26, 'B-PHONE': 25, 'S-EMAIL': 24, 'B-LOC': 22, 'E-LOC': 22, 'I-PHONE': 15, 'S-COMPANY': 13, 'S-JOB': 11, 'S-PHONE': 4})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create NER tagger\n",
        "from flair.embeddings import WordEmbeddings, StackedEmbeddings, TransformerWordEmbeddings\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "embeddings = StackedEmbeddings([\n",
        "                WordEmbeddings('glove')\n",
        "            ])\n",
        "# embeddings = TransformerWordEmbeddings('bert-base-cased',\n",
        "#                                       fine_tune=True,\n",
        "#                                       layers='-1',\n",
        "#                                       subtoken_pooling='first')\n",
        "# embeddings = TransformerWordEmbeddings(\n",
        "#     'roberta-base',  # or 'bert-base-uncased'\n",
        "#     fine_tune=True,\n",
        "#     layers='-1,-2,-3,-4',  # Use last 4 layers\n",
        "#     subtoken_pooling='first',\n",
        "#     allow_long_sentences=True\n",
        "# )\n",
        "\n",
        "tagger = SequenceTagger(hidden_size=256,\n",
        "                         embeddings=embeddings,\n",
        "                         tag_dictionary=tag_dictionary,\n",
        "                         tag_type='ner',\n",
        "                         use_crf=True,\n",
        "                         tag_format = 'BIOES')"
      ],
      "metadata": {
        "id": "QEBUKmiAnaFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc5f4d9-3d3a-4a53-e08e-d8bdb025c732"
      },
      "id": "QEBUKmiAnaFm",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 08:50:12,758 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpqay9flwa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 153M/153M [00:08<00:00, 19.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 08:50:21,189 copying /tmp/tmpqay9flwa to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 08:50:21,625 removing temp file /tmp/tmpqay9flwa\n",
            "2024-11-05 08:50:22,049 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim not found in cache, downloading to /tmp/tmp6c2wvsx4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:01<00:00, 11.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 08:50:24,369 copying /tmp/tmp6c2wvsx4 to cache at /root/.flair/embeddings/glove.gensim\n",
            "2024-11-05 08:50:24,417 removing temp file /tmp/tmp6c2wvsx4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 08:50:31,122 SequenceTagger predicts: Dictionary with 45 tags: O, S-SKILL, B-SKILL, E-SKILL, I-SKILL, S-JOB, B-JOB, E-JOB, I-JOB, S-WORK, B-WORK, E-WORK, I-WORK, S-COMPANY, B-COMPANY, E-COMPANY, I-COMPANY, S-LOC, B-LOC, E-LOC, I-LOC, S-UNI, B-UNI, E-UNI, I-UNI, S-DEG, B-DEG, E-DEG, I-DEG, S-NAME, B-NAME, E-NAME, I-NAME, S-PHONE, B-PHONE, E-PHONE, I-PHONE, S-STUDY, B-STUDY, E-STUDY, I-STUDY, S-EMAIL, B-EMAIL, E-EMAIL, I-EMAIL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train flair ner model\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.training_utils import EvaluationMetric\n",
        "\n",
        "trainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "trainer.train(\n",
        "    base_path='flair_output/',\n",
        "    learning_rate=0.01,\n",
        "    mini_batch_size=16,\n",
        "    max_epochs=5,\n",
        "    train_with_dev=False\n",
        ")\n",
        "!cp -r ./flair_output /content/drive/MyDrive/FYP/Implementation/"
      ],
      "metadata": {
        "id": "XbuJ4VjCnoPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38a5c14-e126-4b28-e7c8-5d526a7db64f"
      },
      "id": "XbuJ4VjCnoPU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 08:50:38,014 ----------------------------------------------------------------------------------------------------\n",
            "2024-11-05 08:50:38,018 Model: \"SequenceTagger(\n",
            "  (embeddings): StackedEmbeddings(\n",
            "    (list_embedding_0): WordEmbeddings(\n",
            "      'glove'\n",
            "      (embedding): Embedding(400001, 100)\n",
            "    )\n",
            "  )\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=47, bias=True)\n",
            "  (loss_function): ViterbiLoss()\n",
            "  (crf): CRF()\n",
            ")\"\n",
            "2024-11-05 08:50:38,021 ----------------------------------------------------------------------------------------------------\n",
            "2024-11-05 08:50:38,024 Corpus: 268 train + 30 dev + 75 test sentences\n",
            "2024-11-05 08:50:38,025 ----------------------------------------------------------------------------------------------------\n",
            "2024-11-05 08:50:38,029 Train:  268 sentences\n",
            "2024-11-05 08:50:38,031         (train_with_dev=False, train_with_test=False)\n",
            "2024-11-05 08:50:38,033 ----------------------------------------------------------------------------------------------------\n",
            "2024-11-05 08:50:38,034 Training Params:\n",
            "2024-11-05 08:50:38,036  - learning_rate: \"0.01\" \n",
            "2024-11-05 08:50:38,037  - mini_batch_size: \"16\"\n",
            "2024-11-05 08:50:38,039  - max_epochs: \"5\"\n",
            "2024-11-05 08:50:38,041  - shuffle: \"True\"\n",
            "2024-11-05 08:50:38,043 ----------------------------------------------------------------------------------------------------\n",
            "2024-11-05 08:50:38,045 Plugins:\n",
            "2024-11-05 08:50:38,047  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'\n",
            "2024-11-05 08:50:38,049 ----------------------------------------------------------------------------------------------------\n",
            "2024-11-05 08:50:38,051 Final evaluation on model from best epoch (best-model.pt)\n",
            "2024-11-05 08:50:38,052  - metric: \"('micro avg', 'f1-score')\"\n",
            "2024-11-05 08:50:38,054 ----------------------------------------------------------------------------------------------------\n",
            "2024-11-05 08:50:38,056 Computation:\n",
            "2024-11-05 08:50:38,057  - compute on device: cpu\n",
            "2024-11-05 08:50:38,059  - embedding storage: cpu\n",
            "2024-11-05 08:50:38,060 ----------------------------------------------------------------------------------------------------\n",
            "2024-11-05 08:50:38,062 Model training base path: \"flair_output\"\n",
            "2024-11-05 08:50:38,064 ----------------------------------------------------------------------------------------------------\n",
            "2024-11-05 08:50:38,067 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/flair/trainers/trainer.py:499: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp and flair.device.type != \"cpu\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model\n",
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.models import SequenceTagger\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "# Load the trained model\n",
        "model = SequenceTagger.load('/content/drive/MyDrive/FYP/Implementation/flair_output/best-model.pt')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "result = model.evaluate(corpus.test, gold_label_type='ner', mini_batch_size=32)\n",
        "\n",
        "# Print the results\n",
        "# print(\"Evaluation Loss:\", eval_loss)\n",
        "print(result.detailed_results)  # print the precision, recall, and F1-score per entity type"
      ],
      "metadata": {
        "id": "6KT8wj3Lns2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6659d061-142d-4b66-a2ed-48b12af52ad5"
      },
      "id": "6KT8wj3Lns2c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-04 13:15:16,427 SequenceTagger predicts: Dictionary with 47 tags: O, S-SKILL, B-SKILL, E-SKILL, I-SKILL, S-JOB, B-JOB, E-JOB, I-JOB, S-WORK, B-WORK, E-WORK, I-WORK, S-COMPANY, B-COMPANY, E-COMPANY, I-COMPANY, S-UNI, B-UNI, E-UNI, I-UNI, S-DEG, B-DEG, E-DEG, I-DEG, S-PHONE, B-PHONE, E-PHONE, I-PHONE, S-NAME, B-NAME, E-NAME, I-NAME, S-STUDY, B-STUDY, E-STUDY, I-STUDY, S-LOC, B-LOC, E-LOC, I-LOC, S-EMAIL, B-EMAIL, E-EMAIL, I-EMAIL, <START>, <STOP>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:03<00:00,  1.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results:\n",
            "- F-score (micro) 0.0055\n",
            "- F-score (macro) 0.003\n",
            "- Accuracy 0.0029\n",
            "\n",
            "By class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       SKILL     0.0000    0.0000    0.0000       429\n",
            "        WORK     0.0261    0.0462    0.0333        65\n",
            "       STUDY     0.0000    0.0000    0.0000        17\n",
            "       PHONE     0.0000    0.0000    0.0000        18\n",
            "         JOB     0.0000    0.0000    0.0000        65\n",
            "     COMPANY     0.0000    0.0000    0.0000        51\n",
            "       EMAIL     0.0000    0.0000    0.0000        12\n",
            "         LOC     0.0000    0.0000    0.0000         9\n",
            "        NAME     0.0000    0.0000    0.0000        21\n",
            "         DEG     0.0000    0.0000    0.0000        17\n",
            "         UNI     0.0000    0.0000    0.0000        17\n",
            "\n",
            "   micro avg     0.0082    0.0042    0.0055       721\n",
            "   macro avg     0.0024    0.0042    0.0030       721\n",
            "weighted avg     0.0024    0.0042    0.0030       721\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make prediction\n",
        "import flair\n",
        "model = SequenceTagger.load('/content/drive/MyDrive/FYP/Implementation/flair_output/best-model.pt')\n",
        "resume_text = '''\n",
        "John Doe lives at 1234 Elm Street in Los Angeles, CA 90001. He can be reached at +1 (555) 123-4567 or via email at john.doe@example.com. John is a results-driven software engineer with over 5 years of experience in web development and cloud infrastructure, with strong knowledge of JavaScript, Python, and cloud technologies like AWS and Azure. Currently, he works as a Software Engineer at Google LLC in San Francisco, CA, where he has been employed since August 2019. In this role, he has developed scalable web applications using JavaScript, Node.js, and React, deployed and maintained cloud infrastructure on AWS, reducing downtime by 20%, and led a team of 4 engineers to enhance backend performance by 30%. Previously, he worked as a Junior Developer at Tech Innovators Inc. in Austin, TX, from July 2017 to July 2019, where he created RESTful APIs using Python and Flask, collaborated with front-end developers to build and deploy user-facing applications, and wrote unit and integration tests, improving code coverage by 15%.\n",
        "\n",
        "John holds a Master of Science in Computer Science from the University of California, Berkeley, with a graduation date of May 2017, and a Bachelor of Science in Information Technology from the University of Texas at Austin, graduated in May 2015. His skillset includes proficiency in programming languages like Python, JavaScript, and Java; frameworks such as React, Flask, and Django; cloud platforms including AWS, Google Cloud, and Azure; as well as other tools like Git, Docker, Kubernetes, and SQL. He is certified as an AWS Certified Solutions Architect – Associate, earned in 2020, and as a Google Professional Cloud Architect, earned in 2021'\n",
        "'''\n",
        "sentence = flair.data.Sentence(resume_text.lower())\n",
        "\n",
        "model.predict(sentence)\n",
        "\n",
        "print(sentence.to_tagged_string())"
      ],
      "metadata": {
        "id": "KOYkWF9Anwec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba81a63-46e2-45b7-d87e-9397a91b858e"
      },
      "id": "KOYkWF9Anwec",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-04 13:15:35,460 SequenceTagger predicts: Dictionary with 47 tags: O, S-SKILL, B-SKILL, E-SKILL, I-SKILL, S-JOB, B-JOB, E-JOB, I-JOB, S-WORK, B-WORK, E-WORK, I-WORK, S-COMPANY, B-COMPANY, E-COMPANY, I-COMPANY, S-UNI, B-UNI, E-UNI, I-UNI, S-DEG, B-DEG, E-DEG, I-DEG, S-PHONE, B-PHONE, E-PHONE, I-PHONE, S-NAME, B-NAME, E-NAME, I-NAME, S-STUDY, B-STUDY, E-STUDY, I-STUDY, S-LOC, B-LOC, E-LOC, I-LOC, S-EMAIL, B-EMAIL, E-EMAIL, I-EMAIL, <START>, <STOP>\n",
            "Sentence[326]: \" john doe lives at 1234 elm street in los angeles, ca 90001. he can be reached at +1 (555) 123-4567 or via email at john.doe@example.com. john is a results-driven software engineer with over 5 years of experience in web development and cloud infrastructure, with strong knowledge of javascript, python, and cloud technologies like aws and azure. currently, he works as a software engineer at google llc in san francisco, ca, where he has been employed since august 2019. in this role, he has developed scalable web applications using javascript, node.js, and react, deployed and maintained cloud infrastructure on aws, reducing downtime by 20%, and led a team of 4 engineers to enhance backend performance by 30%. previously, he worked as a junior developer at tech innovators inc. in austin, tx, from july 2017 to july 2019, where he created restful apis using python and flask, collaborated with front-end developers to build and deploy user-facing applications, and wrote unit and integration tests, improving code coverage by 15%.  john holds a master of science in computer science from the university of california, berkeley, with a graduation date of may 2017, and a bachelor of science in information technology from the university of texas at austin, graduated in may 2015. his skillset includes proficiency in programming languages like python, javascript, and java; frameworks such as react, flask, and django; cloud platforms including aws, google cloud, and azure; as well as other tools like git, docker, kubernetes, and sql. he is certified as an aws certified solutions architect – associate, earned in 2020, and as a google professional cloud architect, earned in 2021'\" → [\"john\"/WORK, \"2021\"/STUDY, \"'\"/EMAIL]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "\n",
        "# Load the pretrained NER model\n",
        "tagger = SequenceTagger.load(\"/content/drive/MyDrive/FYP/Implementation/flair_output/best-model.pt\")  # You can also try \"ner-fast\" for speed\n",
        "# Example text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "# Create a Sentence object\n",
        "sentence = Sentence(resume_text)\n",
        "\n",
        "# Predict entities\n",
        "tagger.predict(sentence)\n",
        "\n",
        "# Print the detected entities\n",
        "for entity in sentence.get_spans(\"ner\"):\n",
        "    print(f\"Entity: {entity.text}, Type: {entity.get_label('ner').value}, Confidence: {entity.score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh_ohqkhnuwA",
        "outputId": "b2bfe420-ac54-4055-fc1c-a4029fd2a270"
      },
      "id": "Eh_ohqkhnuwA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-04 13:15:43,247 SequenceTagger predicts: Dictionary with 47 tags: O, S-SKILL, B-SKILL, E-SKILL, I-SKILL, S-JOB, B-JOB, E-JOB, I-JOB, S-WORK, B-WORK, E-WORK, I-WORK, S-COMPANY, B-COMPANY, E-COMPANY, I-COMPANY, S-UNI, B-UNI, E-UNI, I-UNI, S-DEG, B-DEG, E-DEG, I-DEG, S-PHONE, B-PHONE, E-PHONE, I-PHONE, S-NAME, B-NAME, E-NAME, I-NAME, S-STUDY, B-STUDY, E-STUDY, I-STUDY, S-LOC, B-LOC, E-LOC, I-LOC, S-EMAIL, B-EMAIL, E-EMAIL, I-EMAIL, <START>, <STOP>\n",
            "Entity: John, Type: WORK, Confidence: 0.10987725108861923\n",
            "Entity: 2021, Type: STUDY, Confidence: 0.08692201226949692\n",
            "Entity: ', Type: EMAIL, Confidence: 0.07593147456645966\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}